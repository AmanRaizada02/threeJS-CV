# -*- coding: utf-8 -*-
"""Evaluation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M5ifMS7rNVExv6yMkMavw4NYQ-w4buCF
"""

!pip install pyspark

from pyspark.sql import SparkSession

spark = SparkSession.builder.master("local[*]").getOrCreate()

spark

sc = spark.sparkContext

RDD = [1, 5, 9, 4, 7, 2]

rdd = sc.parallelize(RDD)

subset1 = rdd.take(2)
subset2 = rdd.filter(lambda x: x % 2 == 0).take(2)
subset3 = rdd.filter(lambda x: x % 2 != 0).take(2)


print(subset1)
print(subset2)
print(subset3)

data1=[(1,"Alice",None),(3,"Carol",26),(5,"John",None)]
data2 = [(101, 3, None), (103, 5, 2000), (105, 7, 4000)]

schema1 = ["Customer_id", "Name", "Age"]
schema2 = ["order_id", "Customer_id", "Amount"]

df1 = spark.createDataFrame(data1, schema1)
df2 = spark.createDataFrame(data2, schema2)

result_a = df1.join(df2, df1.Customer_id == df2.Customer_id, "full")

from pyspark.sql.functions import col

df1_selected = df1.select(
    col("Customer_id").alias("customer_id_1"),
    col("Name").alias("name_1"),
    col("Age").alias("age_1")
)

df2_selected = df2.select(
    col("order_id"),
    col("Customer_id").alias("customer_id_2"),
    col("Amount")
)

result_a = df1_selected.join(df2_selected, df1_selected.customer_id_1 == df2_selected.customer_id_2, "full")

result_a = result_a.orderBy(result_a.Amount.desc())

result_a.show()

result_b = df1_selected.join(df2_selected, df1_selected.customer_id_1 == df2_selected.customer_id_2, "right")

result_b = result_b.select(
    col("customer_id_1").alias("customer_id"),
    col("name_1").alias("name"),
    col("age_1").alias("age"),
    col("order_id"),
    col("customer_id_2").alias("id"),
    col("Amount")
)

result_b = result_b.orderBy(result_b.Amount.desc())

result_b.show()

from pyspark.sql import functions as F

# Ensure DataFrame is not empty
if df1.count() > 0:
    non_null_count = df1.filter(F.col("Age").isNotNull()).count()
    sorted_df = df1.sort("Age")

    if non_null_count % 2 == 1:
        median = sorted_df.collect()[non_null_count // 2]["Age"]
    else:
        lower_value = sorted_df.collect()[(non_null_count - 1) // 2]["Age"]
        upper_value = sorted_df.collect()[non_null_count // 2]["Age"]
        median = (lower_value + upper_value) / 2

    # Ensure median is not null and matches column typefrom pyspark.sql import functions as F

# For df1's Age column
non_null_count_age = df1.filter(F.col("Age").isNotNull()).count()
sorted_df_age = df1.sort("Age")

if non_null_count_age % 2 == 1:
    median_age = sorted_df_age.collect()[non_null_count_age // 2]["Age"]
else:
    lower_value_age = sorted_df_age.collect()[(non_null_count_age - 1) // 2]["Age"]
    upper_value_age = sorted_df_age.collect()[non_null_count_age // 2]["Age"]
    median_age = (lower_value_age + upper_value_age) / 2

df1 = df1.fillna({"Age": int(median_age)})

# For df2's Amount column
non_null_count_amount = df2.filter(F.col("Amount").isNotNull()).count()
sorted_df_amount = df2.sort("Amount")

if non_null_count_amount % 2 == 1:
    median_amount = sorted_df_amount.collect()[non_null_count_amount // 2]["Amount"]
else:
    lower_value_amount = sorted_df_amount.collect()[(non_null_count_amount - 1) // 2]["Amount"]
    upper_value_amount = sorted_df_amount.collect()[non_null_count_amount // 2]["Amount"]
    median_amount = (lower_value_amount + upper_value_amount) / 2

df2 = df2.fillna({"Amount": int(median_amount)})

# Display the updated DataFrames
df1.show()
df2.show()
    if median is not None:
        median = int(median)  # Convert median to integer if the Age column is integer
        df1 = df1.fillna({"Age": median})

df1.show()

